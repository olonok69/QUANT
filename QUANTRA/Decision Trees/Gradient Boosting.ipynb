{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Notebook Instructions\n",
    "<i>You can run the notebook document sequentially (one cell a time) by pressing <b> shift + enter</b>. While a cell is running, a [*] will display on the left. When it has been run, a number will display indicating the order in which it was run in the notebook [8].</i>\n",
    "\n",
    "<i>Enter edit mode by pressing <b>`Enter`</b> or using the mouse to click on a cell's editor area. Edit mode is indicated by a green cell border and a prompt showing in the editor area.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "The AdaBoosting framework was further developed by Friedman and was called gradient boosting. In gradient boosting, models are added to the final model that reduces the loss, that is the new model follows gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create a GradientBoostingRegressor model\n",
    "\n",
    "We have used a GradientBoostingRegressor function from the sklearn.ensemble module to create a ada boosting model. The GradientBoostingRegressor takes following as input parameter\n",
    "1. <b>n_estimators:</b> number of trees to create \n",
    "3. <b>random_state:</b> random seed\n",
    "\n",
    "You can use the help function to see the details of GradientBoostingRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=4, presort='auto', random_state=42,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the gradient boosting method\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the gradientboost model\n",
    "gradientboost = GradientBoostingRegressor(\n",
    "                n_estimators=4,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "gradientboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Uncomment below line to see details of GradientBoostingRegressor\n",
    "# help(GradientBoostingRegressor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (SageMath)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}